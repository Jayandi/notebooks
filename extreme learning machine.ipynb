{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Learning Machine (ELM)\n",
    "### Neural network tanpa optimasi iteratif\n",
    "\n",
    "Pada dasarnya, ELM adalah *feed forward neural network* (NN). Berikut ini adalah bentuk paling sederhana dari NN dengan suatu fungsi aktivasi nonlinear, $\\sigma(\\cdot)$:\n",
    "$$\\widehat{\\mathbf{Y}}=\\mathbf{W}_2\\sigma(\\mathbf{W}_1\\mathbf{X})$$\n",
    "\n",
    "Di sini, kita membuat prediksi ($\\widehat{\\mathbf{Y}}\\in\\mathbb{R}^{m \\times c}$) bermodal suatu input ($\\mathbf{X}\\in\\mathbb{R}^{m \\times n}$), di mana $\\mathbf{W}_1\\in\\mathbb{R}^{n \\times h}$ adalah matriks bobot input (untuk mengkoneksikan input ke *hidden* layer), serta $\\mathbf{W}_2\\in\\mathbb{R}^{h \\times c}$ adalah matriks bobot output (untuk mengkoneksikan *hidden* layer (terakhir) ke output). Yang menarik (dan membedakannya dengan NN \"pada umumnya\") adalah:\n",
    "- Matriks bobot input ke *hidden* layer (serta bobot hidden layer ke hidden layer selanjutnya, jika ada beberapa layer) tidak perlu dioptimasi. Dengan kata lain, cukup diacak saja. Sekilas nampak aneh, namun percayalah, ini berfungsi dengan cukup baik :p\n",
    "- Optimasi tidak dilakukan secara iteratif seperti gradient descent dan sejenisnya, namun cukup dengan satu langkah saja: dengan **pseudoinverse**\n",
    "\n",
    "#### Klasifikasi dataset *fisher iris* dengan ELM\n",
    "\n",
    "![Iris_versicolor](https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Iris_versicolor_3.jpg/440px-Iris_versicolor_3.jpg)\n",
    "\n",
    "Sebagai contoh, kita akan menggunakan dataset bunga iris. Dataset ini memiliki 4 *feature* (panjang sepal, lebar sepal, panjang daun bunga, dan lebar daun bunga) dan 3 kelas (*Iris setosa, Iris virginica,* dan *Iris versicolor*). Dataset cukup dibagi menjadi 2: training set (80% dari total data) dan testing set (20% dari total data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# demi kesamaan hasil\n",
    "# 123 dipilih berdasarkan mood penulis\n",
    "np.random.seed(123)\n",
    "\n",
    "X, y                    = load_iris(return_X_y=True)\n",
    "X_tr, X_te, y_tr, y_te  = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Konon, neural net akan bekerja lebih baik dengan\n",
    "# data yang dinormalisasi\n",
    "scaler = StandardScaler().fit(X_tr)\n",
    "X_tr   = scaler.transform(X_tr)\n",
    "X_te   = scaler.transform(X_te)\n",
    "\n",
    "# jumlah baris dan kolom (feature) input\n",
    "m, n                    = X_tr.shape\n",
    "\n",
    "# sebagai contoh, jumlah neuron dalam hidden layer adalah 10\n",
    "h                       = 10\n",
    "\n",
    "# dictionary untuk mengkonversi kode numerik kelas menjadi string\n",
    "name_by_code            = {\n",
    "                            0: 'Iris setosa',\n",
    "                            1: 'Iris virginica',\n",
    "                            2: 'Iris versicolor',\n",
    "                          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biasanya label *ground truth* di-*encode* terlebih dahulu dalam format one-hot encoding. Misal:\n",
    "- Kelas 0 (Iris setosa) di-*encode* menjadi \\[0,0,1\\]\n",
    "- Kelas 1 (Iris virginica) di-*encode* menjadi \\[0,1,0\\]\n",
    "- Kelas 2 (Iris versicolor) di-*encode* menjadi \\[1,0,0\\]\n",
    "\n",
    "Tujuan *encode* adalah agar hasil prediksi bisa diterjemahkan sebagai nilai *confidence* atau tingkat keyakinan (penejelasan *confidence* di bagian akhir). Sklearn sudah menyediakan fungsi one-hot encoding dengan menggunakan kelas LabelBinarizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh label asli: [2 2 0]\n",
      "Contoh label ter-encode: [[0, 0, 1], [0, 0, 1], [1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "y_tr_bin = LabelBinarizer().fit_transform(y_tr)\n",
    "y_te_bin = LabelBinarizer().fit_transform(y_te)\n",
    "\n",
    "# jumlah kolom matriks output\n",
    "h        = y_tr_bin.shape[1]\n",
    "\n",
    "print(\"Contoh label asli:\",y_tr[:3])\n",
    "print(\"Contoh label ter-encode:\",y_tr_bin[:3].tolist())\n",
    "\n",
    "# demi konsistensi dengan formula matematika, dataset akan direpresentasikan sebagai column-vector.\n",
    "X_tr, X_te, y_tr_bin, y_te_bin = X_tr.T, X_te.T, y_tr_bin.T, y_te_bin.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah dataset siap, langkah pertama adalah menyiapkan matriks bobot input. Seperti yang sudah disebutkan, matriks bobot input cukup diacak. Sebagai contoh, kita menggunakan nilai acak dengan distribusi standard normal, $\\mathbf{W}_1\\sim\\mathcal{N}(0, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1 berukuran h x n. Artinya, dia akan memetakan input X yang berdimensi n menjadi\n",
    "# hidden representation H berdimensi h\n",
    "W1 = np.random.randn(h, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langkah selanjutnya adalah estimasi $\\mathbf{W}_2$. Pada tahap ini, kita sedang melakukan training. Misal, label ter-*encode* pada training dataset direpresentasikan sebagai $\\mathbf{Y}$. Substitusikan dalam persamaan, kita akan memperoleh:\n",
    "$$\\mathbf{Y}=\\mathbf{W}_2\\sigma(\\mathbf{W}_1\\mathbf{X})$$\n",
    "Maka, $\\mathbf{W}_2$ optimal dapat diperoleh sebagai berikut:\n",
    "$$\\widehat{\\mathbf{W}}_2=\\mathbf{Y}\\sigma(\\mathbf{W}_1\\mathbf{X})^+$$\n",
    "*Superscript* \"+\" menandakan operasi (Moore-Penrose) pseudoinverse. Pseudoinverse suatu matriks bisa diperoleh dengan prosedur yang melibatkan *singular value decomposition* (SVD). Namun, numpy sudah menyediakan fungsi pinv() untuk menghitung pseudoinverse.\n",
    "\n",
    "Untuk fungsi aktivasi, pada contoh ini kita akan menggunakan fungsi sigmoid:\n",
    "\n",
    "$$\\sigma(x)=\\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi sigmoid\n",
    "def sig(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# estimasi (a.k.a. training) W2 dengan pseudoinverse\n",
    "W2 = y_tr_bin @ np.linalg.pinv(sig(W1 @ X_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampai di sini kita sudah bisa melakukan prediksi (*inference*):\n",
    "$$\\widehat{\\mathbf{Y}}=\\widehat{\\mathbf{W}}_2\\sigma(\\mathbf{W}_1\\mathbf{X})$$\n",
    "\n",
    "Sebagai contoh, kita mempunyai data bunga iris baru sbb:\n",
    "- panjang sepal = 5.2 cm\n",
    "- lebar sepal = 3.5 cm\n",
    "- panjang daun bunga = 4 cm\n",
    "- lebar daun bunga = 2 cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediksi kelas: 2 (Iris versicolor)\n"
     ]
    }
   ],
   "source": [
    "X_new = np.array([[5.2, 3.5, 4.0, 2.0]]).T\n",
    "y_hat = (W2 @ sig(W1 @ X_new))\n",
    "\n",
    "label = y_hat.argmax()\n",
    "\n",
    "print('prediksi kelas: %d (%s)' % (label, name_by_code[label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benarkah? Entahlah. Itu hanya data contoh. Untuk mengevaluasi performa (akurasi) model, kita bisa menggunakan testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi ELM: 93.33%\n"
     ]
    }
   ],
   "source": [
    "pred = (W2@sig(W1 @ X_te)).argmax(axis=0)\n",
    "acc  = np.mean([pred == y_te])\n",
    "print('Akurasi ELM: %.2f%%' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mari kita coba bandingkan dengan neural network bawaan sklearn dengan parameter default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi neural net (sklearn): 93.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghora/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier().fit(X_tr.T, y_tr)\n",
    "acc = np.mean([clf.predict(X_te.T) == y_te])\n",
    "print('Akurasi neural net (sklearn): %.2f%%' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ternyata tidak jauh berbeda. Cukup bagus lah, terlebih model kita sangat sederhana. Namun ada poin yang menjadi kelebihan ELM: Tidak membutuhkan update matrix bobot secara iteratif. Artinya? Kita tidak perlu bingung memilih berapa iterasi hingga konvergen. Kita juga tidak bingung memilih nilai *learning rate* yang pas.\n",
    "\n",
    "Perlu diingat, pseudoinverse adalah operasi yang semakin berat saat jumlah data semakin banyak. Untuk melakukan pseudoinverse, semua data harus dimuat di memori terlebih dahulu. Saat data sudah sangat besar, sebaiknya kita beralih pada metode iteratif ala algoritma gradient descent dan variannya, karena algoritma tsb mampu melakukan optimasi tanpa harus memuat seluruh data di memori (i.e., dengan sistem *mini-batching*)\n",
    "\n",
    "#### Seputar interpretasi output ELM\n",
    "\n",
    "Seperti dijelaskan di awal, kita tidak menggunakan label asli, melainkan versi ter-*encode* label tsb. Tujuan *encode* adalah agar hasil prediksi bisa direpresentasikan sebagai vektor berisi nilai *confidence* (tingkat keyakinan) masing-masing kelas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output prediksi:\n",
      "[[ 0.19635205]\n",
      " [-0.18852528]\n",
      " [ 0.64047678]]\n"
     ]
    }
   ],
   "source": [
    "X_new = np.array([[5.2, 3.5, 4.0, 2.0]]).T\n",
    "y_hat = (W2 @ sig(W1 @ X_new))\n",
    "\n",
    "print('Output prediksi:')\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oooops... Secara default, output ELM tidak bisa secara langsung diinterpretasikan sebagai *confidence* karena output tsb masih bukanlah distribusi probabilitas (i.e., elemen-elemen vektor output prediksi di atas jika dijumlahkan != 1), walau toh label final tetap bisa ditebak dengan mengambil indeks baris dengan nilai terbesar (indeks 2, mewakili kelas 2 (Iris versicolor)).\n",
    "\n",
    "Kita bisa menggunakan fungsi softmax untuk memperoleh *confidence* yang (lebih) tepat:\n",
    "$$softmax(\\mathbf{x})=\\frac{exp(\\mathbf{x})}{\\sum exp(\\mathbf{x})}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output prediksi:\n",
      "[[0.30867446]\n",
      " [0.2100635 ]\n",
      " [0.48126204]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x), axis=0)\n",
    "\n",
    "X_new = np.array([[5.2, 3.5, 4.0, 2.0]]).T\n",
    "y_hat = (W2 @ sig(W1 @ X_new))\n",
    "\n",
    "print('Output prediksi:')\n",
    "print(softmax(y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artinya, data baru tersebut (X_new) merupakan jenis Iris versicolor dengan tingkat keyakinan 0.38.\n",
    "\n",
    "Selamat mencoba."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
